apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}
  labels:
    app: {{ .Release.Name }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app: {{ .Release.Name }}
    spec:
      containers:
      - name: gemma-inference
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        resources:
          limits:
            nvidia.com/gpu: {{ index .Values.resources.limits "nvidia.com/gpu" }}
          requests:
            cpu: {{ .Values.resources.requests.cpu }}
            memory: {{ .Values.resources.requests.memory }}
        ports:
        - containerPort: 8000
          name: http
        
        env:
        {{- range $key, $value := .Values.env }}
        - name: {{ $key }}
          value: {{ $value | quote }}
        {{- end }}
        
        # PROBE CONFIGURATION
        # We need generous timeouts for AI models as they load large files into GPU memory
        readinessProbe:
          httpGet:
            path: /
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
          failureThreshold: 20
        livenessProbe:
          httpGet:
            path: /
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 15
          
        # STARTUP SCRIPT FIX
        # This replaces the default container command to fix a known port mismatch issue
        command:
        - "/bin/bash"
        - "-c"
        - |
          set -e
          echo "Starting Ollama server in background..."
          /bin/ollama serve &
          OLLAMA_PID=$!
          
          MAX_RETRIES=50
          RETRY_INTERVAL=2
          retries=0
          
          echo "Waiting for Ollama to be ready on port 8000..."
          until [[ "$retries" -ge "$MAX_RETRIES" ]] || curl -s --fail http://localhost:8000; do
            echo "Attempt $retries: Waiting for Ollama to be ready..."
            sleep "$RETRY_INTERVAL"
            retries=$((retries + 1))
          done
          
          if [[ "$retries" -ge "$MAX_RETRIES" ]]; then
            echo "Ollama did not become ready within the timeout period. Exiting."
            exit 1
          fi
          
          echo "Ollama is ready."
          trap "echo 'Caught SIGTERM, shutting down...'; kill $OLLAMA_PID; wait $OLLAMA_PID" SIGTERM SIGINT
          wait $OLLAMA_PID

      # NODE SCHEDULING
      # Ensures this runs ONLY on the GPU nodes we created
      nodeSelector:
        {{- toYaml .Values.nodeSelector | nindent 8 }}
      tolerations:
        {{- toYaml .Values.tolerations | nindent 8 }}
